{
  "categories": [
    {
      "id": "physics",
      "name": "Physics",
      "color": "#f97316",
      "primitives": [
        {
          "name": "Speed of light",
          "implications": [
            "network latency floor (~1μs PCIe, ~1ms local, ~100ms cross-continent)",
            "you cannot avoid round-trips, only reduce them"
          ],
          "explanation": "<b>Latency has a floor set by physics:</b> ~1μs for PCIe round-trip, ~1ms same-datacenter, ~100ms cross-continent. You can't optimize below this — only amortize it. <b>Every protocol that seems slow</b> is usually paying for round-trips, not computation. This is why <b>batching exists everywhere:</b> the cost is per-transaction, not per-byte."
        },
        {
          "name": "Memory hierarchy",
          "implications": [
            "registers < L1 < L2 < L3 < RAM < SSD < HDD < network",
            "~1ns → ~3ns → ~10ns → ~40ns → ~100ns → ~100μs → ~10ms → ~1-100ms"
          ],
          "explanation": "<b>Each level is ~10x slower and ~10x larger.</b> Algorithms that seem O(n) identical can differ 100x based on access patterns. <b>Cache-oblivious algorithms</b> exist precisely because this hierarchy is universal. On FPGA, the hierarchy extends: BRAM (on-chip) → DDR (on-card) → host RAM (PCIe)."
        },
        {
          "name": "Bandwidth ≠ latency",
          "implications": [
            "high throughput, still pay per-round-trip",
            "pipeline/batch to amortize"
          ],
          "explanation": "You can have <b>100 Gbps and still wait 1ms</b> for first byte. <b>Bandwidth</b> is asymptotic throughput; <b>latency</b> is fixed cost per operation. High bandwidth only helps if you're moving enough data to amortize the latency. <b>Small messages are latency-bound;</b> large transfers are bandwidth-bound."
        },
        {
          "name": "Failure",
          "implications": [
            "everything fails: disks, nodes, networks, datacenters",
            "MTBF matters at scale (1000 disks = daily failures)"
          ],
          "explanation": "<b>At scale, failure is certain.</b> P(at least one failure) = 1 - (1-p)^n. With 1000 nodes at 0.1% daily failure rate, expect ~1 failure/day. <b>Design must assume failure,</b> not prevent it. <b>MTBF</b> determines redundancy requirements. <b>Blast radius</b> determines how much fails together."
        },
        {
          "name": "Energy / memory wall",
          "implications": [
            "computation costs power",
            "moving data costs more than computing on it"
          ],
          "explanation": "<b>Compute is essentially free;</b> moving data costs energy and time. FLOP: ~1 pJ. 64B from DRAM: ~1000 pJ. Modern CPUs spend most transistors on <b>cache, not ALUs.</b> Memory bandwidth is the bottleneck for most workloads. This is why <b>custom hardware (FPGA/ASIC) wins:</b> you build datapaths that keep data local."
        }
      ]
    },
    {
      "id": "cs",
      "name": "CS",
      "color": "#3b82f6",
      "primitives": [
        {
          "name": "Time vs space",
          "implications": [
            "caching, precomputation, indexes, materialized views"
          ],
          "explanation": "<b>Store precomputed results (space)</b> or recompute on demand (time). Every cache, index, and materialized view is this tradeoff. <b>Memoization</b> trades O(n) space for O(1) lookup. <b>Compression</b> trades CPU for storage/bandwidth."
        },
        {
          "name": "Sequential vs random access",
          "implications": [
            "10x-1000x difference (disk, RAM, prefetch)"
          ],
          "explanation": "<b>Sequential is 10-1000x faster than random,</b> everywhere. SSD: ~500K random IOPS vs ~3GB/s sequential. RAM: prefetcher handles sequential, stalls on random. This is why <b>append-only logs, LSM trees, and column stores</b> exist — they turn random writes into sequential."
        },
        {
          "name": "Coordination cost",
          "implications": [
            "total order is expensive (consensus, locks)",
            "partial order cheaper (CRDTs, vector clocks)"
          ],
          "explanation": "<b>Total order requires communication.</b> Locks serialize. Consensus needs quorum round-trips. <b>Partial order</b> (vector clocks, CRDTs) avoids coordination but limits what you can express. <b>The CAP theorem is really:</b> coordination costs latency, and partitions force you to choose."
        },
        {
          "name": "Amortized vs worst-case",
          "implications": [
            "batching helps throughput, hurts tail latency"
          ],
          "explanation": "<b>Batching improves throughput</b> at the cost of tail latency. Amortized O(1) means occasional O(n) spike. For <b>real-time systems,</b> worst-case matters. For <b>throughput systems,</b> amortized matters."
        },
        {
          "name": "Immutability",
          "implications": [
            "enables sharing, caching, replication, rollback",
            "append-only = sequential = fast"
          ],
          "explanation": "<b>Immutable data can be freely shared,</b> cached, replicated, and rolled back. No synchronization needed for readers. <b>Append-only structures</b> (logs, persistent data structures) get these benefits. <b>Copy-on-write</b> gives immutability semantics with mutation performance."
        },
        {
          "name": "Hashing",
          "implications": [
            "O(1) lookup, uniform distribution",
            "but: no ordering, no range queries"
          ],
          "explanation": "<b>O(1) average lookup,</b> uniform distribution. But: <b>no ordering, no range queries,</b> hash collisions in adversarial input. <b>Cryptographic hashes</b> add one-wayness and collision resistance at CPU cost."
        },
        {
          "name": "Compression",
          "implications": [
            "trades CPU for space/bandwidth",
            "entropy is the limit"
          ],
          "explanation": "Trades CPU for space/bandwidth. <b>Entropy is the floor</b> — you can't compress below information content. <b>Dictionary methods (LZ)</b> exploit repetition. <b>Entropy coders</b> (Huffman, ANS) approach entropy limit."
        }
      ]
    },
    {
      "id": "complexity",
      "name": "Complexity",
      "color": "#8b5cf6",
      "primitives": [
        {
          "name": "O(1) vs O(log n) vs O(n)",
          "implications": [
            "index or scan? tree or hash?"
          ],
          "explanation": "<b>Index or scan? Tree or hash?</b> The constant factors matter at small n; the asymptotic wins at large n. But <b>memory access patterns can flip this</b> — O(log n) with cache-friendly access beats O(1) with random access at moderate n."
        },
        {
          "name": "Write amplification",
          "implications": [
            "LSM: batch writes, pay on compaction",
            "B-tree: pay on write, cheaper reads"
          ],
          "explanation": "<b>LSM:</b> writes go to memory buffer, flush sequentially, compact later. Write is O(1) amortized but each record is written multiple times across levels. <b>B-tree:</b> write in place, one write per record, but random I/O. <b>LSM wins write-heavy; B-tree wins read-heavy.</b>"
        },
        {
          "name": "Read vs write optimization",
          "implications": [
            "cannot optimize both fully",
            "normalize (write-friendly) vs denormalize (read-friendly)"
          ],
          "explanation": "<b>You cannot optimize both fully.</b> <b>Normalization:</b> single source of truth, writes are clean, reads require joins. <b>Denormalization:</b> redundant copies, writes must update all copies, reads are fast. Event sourcing is extreme write-optimize; materialized views are read-optimize."
        },
        {
          "name": "Fan-out",
          "implications": [
            "1 request → N downstream",
            "latency = max(children), not sum",
            "tail latency dominates"
          ],
          "explanation": "1 request → N downstream. <b>Latency = max(children), not sum.</b> Tail latency of children dominates. If each child has 1% chance of slow response, <b>with 100 children you're almost certain to be slow.</b>"
        },
        {
          "name": "Fan-in",
          "implications": [
            "N sources → 1 sink",
            "backpressure, bottleneck risk"
          ],
          "explanation": "N sources → 1 sink. <b>Backpressure risk.</b> If sink is slower than combined input rate, queue grows unbounded. Need <b>admission control</b> or <b>load shedding.</b>"
        },
        {
          "name": "Hot spots / skew",
          "implications": [
            "uniform distribution is a fantasy",
            "popular keys, time-based clustering"
          ],
          "explanation": "<b>Uniform distribution is a fantasy.</b> Zipf is everywhere: popular keys, time-based clustering, celebrity users. A \"balanced\" hash ring <b>still has hot spots</b> because some keys are accessed 1000x more."
        },
        {
          "name": "Failure domains",
          "implications": [
            "what dies together?",
            "redundancy must cross failure boundaries"
          ],
          "explanation": "<b>What dies together?</b> Redundancy must cross failure boundaries. Two replicas on same rack: rack switch failure kills both. Two replicas in same AZ: AZ failure kills both. <b>Correlated failures must be modeled.</b>"
        }
      ]
    },
    {
      "id": "distributed",
      "name": "Distributed",
      "color": "#14b8a6",
      "primitives": [
        {
          "name": "CAP",
          "implications": [
            "partition is physics (network fails)",
            "choice: consistency or availability during partition"
          ],
          "explanation": "<b>Partition is physics</b> — network fails. During partition, choose: <b>reject writes (consistency)</b> or <b>accept writes that may diverge (availability).</b> Most systems aren't \"CP or AP\" globally — they make different choices for different operations."
        },
        {
          "name": "Consensus",
          "implications": [
            "total order across nodes = round-trips = latency floor",
            "Paxos/Raft = 2 RTTs minimum"
          ],
          "explanation": "<b>Total order across nodes requires communication.</b> <b>Paxos/Raft:</b> 2 RTT minimum. <b>Leader-based:</b> 1 RTT but leader is bottleneck and single point of failure. <b>Leaderless (EPaxos):</b> more complex, potentially lower latency for non-conflicting operations."
        },
        {
          "name": "Replication",
          "implications": [
            "redundancy for durability/availability",
            "sync (slow, safe) vs async (fast, lossy)"
          ],
          "explanation": "Redundancy for <b>durability</b> (don't lose data) and <b>availability</b> (keep serving). <b>Sync replication:</b> write waits for all replicas, safe but slow. <b>Async replication:</b> write returns immediately, replicas may lag, risk of data loss on failure."
        },
        {
          "name": "Partitioning / sharding",
          "implications": [
            "horizontal scale",
            "cross-partition = coordination cost"
          ],
          "explanation": "<b>Horizontal scale</b> — data split across nodes. But <b>cross-partition operations require coordination.</b> Partition key choice determines what's cheap (same partition) vs expensive (scatter-gather)."
        }
      ]
    },
    {
      "id": "queueing",
      "name": "Queueing",
      "color": "#22c55e",
      "primitives": [
        {
          "name": "Utilization vs latency",
          "implications": [
            "latency explodes as utilization → 100%",
            "rule of thumb: ≤70% for stable latency"
          ],
          "explanation": "<b>Latency explodes as utilization approaches 100%.</b> M/M/1 queue: mean wait = service_time / (1 - utilization). At 90% utilization, wait is 9x service time. At 99%, wait is 99x. <b>Rule of thumb: keep utilization ≤70%</b> for stable latency."
        },
        {
          "name": "Little's Law",
          "implications": [
            "L = λW (items in system = arrival rate × time in system)"
          ],
          "explanation": "<b>L = λW.</b> Items in system = arrival rate × time in system. Works for any stable system. <b>Useful for back-of-envelope capacity planning.</b>"
        }
      ]
    }
  ]
}
